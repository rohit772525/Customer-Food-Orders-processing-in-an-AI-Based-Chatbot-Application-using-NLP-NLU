{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "!pip install transformers\n",
        "!pip install tensorflow\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.callbacks import ProgbarLogger\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/NLP_NEW_DATA (3).csv\", encoding='latin-1')\n",
        "data = data.dropna(subset=[\"Prompt\"])  # Remove rows with missing values in \"Prompt\" column\n",
        "texts = data[\"Prompt\"].tolist()\n",
        "intents = data[[\"IsFood\", \"Cuisine Type\", \"Cuisine Region\", \"Dish Name\"]].values\n",
        "\n",
        "# Encode the intents as numerical labels\n",
        "label_encoders = []\n",
        "encoded_intent s = []\n",
        "for i in range(intents.shape[1]):\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_intent = label_encoder.fit_transform(intents[:, i])\n",
        "    label_encoders.append(label_encoder)\n",
        "    encoded_intents.append(encoded_intent)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = []\n",
        "for i in range(intents.shape[1]):\n",
        "    class_weight = compute_class_weight(\"balanced\", classes=sorted(set(encoded_intents[i])), y=encoded_intents[i])\n",
        "    class_weights.append(class_weight)\n",
        "\n",
        "# Preprocess the data using tf.dat\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "\n",
        "for i in range(intents.shape[1]):\n",
        "    filtered_indices = [idx for idx, intent in enumerate(encoded_intents[i]) if intent != -1]\n",
        "    filtered_texts = [texts[idx] for idx in filtered_indices]\n",
        "    filtered_intents = [encoded_intents[i][idx] for idx in filtered_indices]\n",
        "\n",
        "    train_texts, test_texts, train_intents, test_intents = train_test_split(\n",
        "        filtered_texts,\n",
        "        filtered_intents,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(train_encodings),\n",
        "        train_intents\n",
        "    ))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(test_encodings),\n",
        "        test_intents\n",
        "    ))\n",
        "\n",
        "    train_datasets.append(train_dataset.shuffle(100).batch(16))\n",
        "    test_datasets.append(test_dataset.batch(16))\n",
        "\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "models = []\n",
        "for i in range(intents.shape[1]):\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(set(encoded_intents[i])))\n",
        "    models.append(model)\n",
        "\n",
        "# Fine-tune the models\n",
        "epochs = 5  # Define the number of epochs you want to train for\n",
        "\n",
        "for i in range(intents.shape[1]):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        total_steps = len(train_datasets[i])\n",
        "        current_step = 0\n",
        "\n",
        "        for inputs, labels in train_datasets[i]:\n",
        "            current_step += 1\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = models[i](inputs, training=True)\n",
        "                logits = predictions['logits']  # Extract logits\n",
        "                loss = loss_fn(labels, logits)  # Compute loss using logits\n",
        "\n",
        "            # Compute gradients and update weights\n",
        "            gradients = tape.gradient(loss, models[i].trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, models[i].trainable_variables))\n",
        "\n",
        "            # Update accuracy metric\n",
        "            accuracy_metric.update_state(labels, logits)  # Use logits for accuracy calculation\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"\\rEpoch {epoch+1}/{epochs} - Step {current_step}/{total_steps}\", end=\"\")\n",
        "\n",
        "        # Validation loop\n",
        "        for inputs, labels in test_datasets[i]:\n",
        "            predictions = models[i](inputs, training=False)\n",
        "            logits = predictions['logits']  # Extract logits\n",
        "            accuracy_metric.update_state(labels, logits)  # Use logits for accuracy calculation\n",
        "\n",
        "        # Print training and validation accuracy for the current epoch\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}: Training Accuracy = {accuracy_metric.result().numpy()}, Validation Accuracy = {accuracy_metric.result().numpy()}\")\n",
        "\n",
        "        # Reset accuracy metric for the next epoch\n",
        "        accuracy_metric.reset_states()\n",
        "\n",
        "    print(f\"Completed Epoch {epoch+1} for intent {i+1}\")\n",
        "    models[i].save_pretrained(f\"BERT_V1\")\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Initialize lists to store the true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test dataset to collect true and predicted labels\n",
        "for inputs, labels in test_datasets[i]:\n",
        "    predictions = models[i](inputs, training=False)\n",
        "    logits = predictions['logits']\n",
        "    predicted_label = tf.argmax(logits, axis=1).numpy()\n",
        "\n",
        "    true_labels.extend(labels.numpy())\n",
        "    predicted_labels.extend(predicted_label)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Example text prompts\n",
        "text_prompts = [\n",
        "    \"Can you bring me some Dahi Puri?\",\n",
        "    \"Can you bring me some Cultery?\",\n",
        "    \"Can you bring me some Tandoori Chicken?\",\n",
        "    \"Can you bring me some Manchurian?\",\n",
        "    \"Can you bring me some Chicken Tikka Sandwich?\",\n",
        "    \"Can you bring me some Chicken 65?\",\n",
        "    \"Can you bring me some Butter Naan?\",\n",
        "    \"Can you bring me some Dum Biryani?\",\n",
        "    \"Can you bring me some Salad?\",\n",
        "    \"Can you bring me some Pizza?\",\n",
        "    \"Can you bring me some Bread?\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text prompts\n",
        "tokenized_prompts = [tokenizer.encode_plus(\n",
        "    prompt,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors=\"tf\"\n",
        ") for prompt in text_prompts]\n",
        "\n",
        "# Get the input IDs from the tokenized prompts\n",
        "input_ids_list = [prompt[\"input_ids\"] for prompt in tokenized_prompts]\n",
        "\n",
        "# Make predictions using the trained models\n",
        "predicted_labels_list = []\n",
        "for i in range(intents.shape[1]):\n",
        "    predicted_labels = []\n",
        "    for input_ids in input_ids_list:\n",
        "        predictions = models[i].predict(input_ids)\n",
        "        logits = predictions['logits']  # Extract logits\n",
        "        predicted_label = label_encoders[i].inverse_transform(tf.argmax(logits, axis=1).numpy())\n",
        "        predicted_labels.append(predicted_label[0])\n",
        "    predicted_labels_list.append(predicted_labels)\n",
        "\n",
        "# Print the predicted intent labels for each text prompt\n",
        "for i, prompt in enumerate(text_prompts):\n",
        "    print(f\"Predicted Intent for prompt {i+1}:\")\n",
        "    print(\"IsFood:\", predicted_labels_list[0][i])\n",
        "    print(\"Cuisine Type:\", predicted_labels_list[1][i])\n",
        "    print(\"Cuisine Region:\", predicted_labels_list[2][i])\n",
        "    print(\"Dish Name:\", predicted_labels_list[3][i])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6AKPWMKSwZA",
        "outputId": "8783a702-32cc-4bb9-b582-ef71707f5957"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Step 81/81\n",
            "Epoch 1/5: Training Accuracy = 0.9956602454185486, Validation Accuracy = 0.9956602454185486\n",
            "Epoch 2/5 - Step 81/81\n",
            "Epoch 2/5: Training Accuracy = 0.9993800520896912, Validation Accuracy = 0.9993800520896912\n",
            "Epoch 3/5 - Step 81/81\n",
            "Epoch 3/5: Training Accuracy = 0.9993800520896912, Validation Accuracy = 0.9993800520896912\n",
            "Epoch 4/5 - Step 81/81\n",
            "Epoch 4/5: Training Accuracy = 0.9993800520896912, Validation Accuracy = 0.9993800520896912\n",
            "Epoch 5/5 - Step 81/81\n",
            "Epoch 5/5: Training Accuracy = 0.9993800520896912, Validation Accuracy = 0.9993800520896912\n",
            "Completed Epoch 5 for intent 1\n",
            "Epoch 1/5 - Step 81/81\n",
            "Epoch 1/5: Training Accuracy = 0.860508382320404, Validation Accuracy = 0.860508382320404\n",
            "Epoch 2/5 - Step 81/81\n",
            "Epoch 2/5: Training Accuracy = 0.9721016883850098, Validation Accuracy = 0.9721016883850098\n",
            "Epoch 3/5 - Step 81/81\n",
            "Epoch 3/5: Training Accuracy = 0.9981400966644287, Validation Accuracy = 0.9981400966644287\n",
            "Epoch 4/5 - Step 81/81\n",
            "Epoch 4/5: Training Accuracy = 0.9987601041793823, Validation Accuracy = 0.9987601041793823\n",
            "Epoch 5/5 - Step 81/81\n",
            "Epoch 5/5: Training Accuracy = 0.9993800520896912, Validation Accuracy = 0.9993800520896912\n",
            "Completed Epoch 5 for intent 2\n",
            "Epoch 1/5 - Step 81/81\n",
            "Epoch 1/5: Training Accuracy = 0.430254191160202, Validation Accuracy = 0.430254191160202\n",
            "Epoch 2/5 - Step 81/81\n",
            "Epoch 2/5: Training Accuracy = 0.7823930382728577, Validation Accuracy = 0.7823930382728577\n",
            "Epoch 3/5 - Step 81/81\n",
            "Epoch 3/5: Training Accuracy = 0.9287042617797852, Validation Accuracy = 0.9287042617797852\n",
            "Epoch 4/5 - Step 81/81\n",
            "Epoch 4/5: Training Accuracy = 0.9677619338035583, Validation Accuracy = 0.9677619338035583\n",
            "Epoch 5/5 - Step 81/81\n",
            "Epoch 5/5: Training Accuracy = 0.9733415842056274, Validation Accuracy = 0.9733415842056274\n",
            "Completed Epoch 5 for intent 3\n",
            "Epoch 1/5 - Step 81/81\n",
            "Epoch 1/5: Training Accuracy = 0.08617483079433441, Validation Accuracy = 0.08617483079433441\n",
            "Epoch 2/5 - Step 81/81\n",
            "Epoch 2/5: Training Accuracy = 0.3998759984970093, Validation Accuracy = 0.3998759984970093\n",
            "Epoch 3/5 - Step 81/81\n",
            "Epoch 3/5: Training Accuracy = 0.7458152770996094, Validation Accuracy = 0.7458152770996094\n",
            "Epoch 4/5 - Step 81/81\n",
            "Epoch 4/5: Training Accuracy = 0.9119653105735779, Validation Accuracy = 0.9119653105735779\n",
            "Epoch 5/5 - Step 81/81\n",
            "Epoch 5/5: Training Accuracy = 0.9665220379829407, Validation Accuracy = 0.9665220379829407\n",
            "Completed Epoch 5 for intent 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9604\n",
            "Recall: 0.9752\n",
            "F1 Score: 0.9659\n",
            "Confusion Matrix:\n",
            "[[8 0 0 ... 0 0 0]\n",
            " [0 7 0 ... 0 0 0]\n",
            " [0 0 8 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 8 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 3]]\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "Predicted Intent for prompt 1:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: West India\n",
            "Dish Name: Dahi Puri\n",
            "\n",
            "Predicted Intent for prompt 2:\n",
            "IsFood: Yes\n",
            "Cuisine Type: General\n",
            "Cuisine Region: General\n",
            "Dish Name: General\n",
            "\n",
            "Predicted Intent for prompt 3:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: General\n",
            "Dish Name: Tandoori Chiken\n",
            "\n",
            "Predicted Intent for prompt 4:\n",
            "IsFood: Yes\n",
            "Cuisine Type: General\n",
            "Cuisine Region: General\n",
            "Dish Name: Manchurian\n",
            "\n",
            "Predicted Intent for prompt 5:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: West India\n",
            "Dish Name: Chiken Tikka Sandwish\n",
            "\n",
            "Predicted Intent for prompt 6:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: South India\n",
            "Dish Name: Chiken 65\n",
            "\n",
            "Predicted Intent for prompt 7:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: South India\n",
            "Dish Name: Butter Naan\n",
            "\n",
            "Predicted Intent for prompt 8:\n",
            "IsFood: Yes\n",
            "Cuisine Type: General\n",
            "Cuisine Region: General\n",
            "Dish Name: Dum Biryani\n",
            "\n",
            "Predicted Intent for prompt 9:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: General\n",
            "Dish Name: Salad\n",
            "\n",
            "Predicted Intent for prompt 10:\n",
            "IsFood: Yes\n",
            "Cuisine Type: General\n",
            "Cuisine Region: General \n",
            "Dish Name: Pizza\n",
            "\n",
            "Predicted Intent for prompt 11:\n",
            "IsFood: Yes\n",
            "Cuisine Type: Indian\n",
            "Cuisine Region: General\n",
            "Dish Name: Bread\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):  # Assuming you have 4 intent categories\n",
        "    # ... (rest of the code)\n",
        "\n",
        "    # Iterate through the test dataset to collect true and predicted labels\n",
        "    for inputs, labels in test_datasets[i]:\n",
        "        predictions = models[i](inputs, training=False)  # Corrected variable name here\n",
        "        logits = predictions['logits']\n",
        "        predicted_label = tf.argmax(logits, axis=1).numpy()\n",
        "\n",
        "        true_labels.extend(labels.numpy())\n",
        "        predicted_labels.extend(predicted_label)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"PrAecision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "\n",
        "    # Print the entire confusion matrix\n",
        "    print(\"Full Confusion Matrix:\")\n",
        "    print(confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfelGfur7_hZ",
        "outputId": "a70a09fa-4873-4d15-9db4-4011d9ba4d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9938\n",
            "Recall: 0.9969\n",
            "F1 Score: 0.9954\n",
            "Confusion Matrix:\n",
            "[[  0   1]\n",
            " [  0 322]]\n",
            "Full Confusion Matrix:\n",
            "[[  0   1]\n",
            " [  0 322]]\n",
            "Precision: 0.9985\n",
            "Recall: 0.9985\n",
            "F1 Score: 0.9984\n",
            "Confusion Matrix:\n",
            "[[ 61   1]\n",
            " [  0 584]]\n",
            "Full Confusion Matrix:\n",
            "[[ 61   1]\n",
            " [  0 584]]\n",
            "Precision: 0.9896\n",
            "Recall: 0.9897\n",
            "F1 Score: 0.9893\n",
            "Confusion Matrix:\n",
            "[[105   1   0   0   0   0   0   0]\n",
            " [  0 709   0   1   0   0   0   0]\n",
            " [  0   0  13   0   0   0   0   0]\n",
            " [  0   8   0  38   0   0   0   0]\n",
            " [  0   0   0   0  24   0   0   0]\n",
            " [  0   0   0   0   0  14   0   0]\n",
            " [  0   0   0   0   0   0  17   0]\n",
            " [  0   0   0   0   0   0   0  39]]\n",
            "Full Confusion Matrix:\n",
            "[[105   1   0   0   0   0   0   0]\n",
            " [  0 709   0   1   0   0   0   0]\n",
            " [  0   0  13   0   0   0   0   0]\n",
            " [  0   8   0  38   0   0   0   0]\n",
            " [  0   0   0   0  24   0   0   0]\n",
            " [  0   0   0   0   0  14   0   0]\n",
            " [  0   0   0   0   0   0  17   0]\n",
            " [  0   0   0   0   0   0   0  39]]\n",
            "Precision: 0.9823\n",
            "Recall: 0.9861\n",
            "F1 Score: 0.9835\n",
            "Confusion Matrix:\n",
            "[[113   1   0 ...   0   0   0]\n",
            " [  0 716   0 ...   0   0   0]\n",
            " [  0   0  21 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ...   8   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   3]]\n",
            "Full Confusion Matrix:\n",
            "[[113   1   0 ...   0   0   0]\n",
            " [  0 716   0 ...   0   0   0]\n",
            " [  0   0  21 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ...   8   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   3]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}